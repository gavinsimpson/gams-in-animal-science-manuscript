---
title: "Method: Using generalized additive models in the animal sciences"
authors:
  - name: G. L. Simpson
    id: gls
    orcid: 0000-0002-9084-8413
    email: gavin@anivet.au.dk
    affiliations: 
      - name: Aarhus University
        department: Department of Animal and Veterinary Sciences
        address: Blichers AllÃ© 20
        city: Tjele
        postal-code: 8830
        country: Denmark
abstract: > 
  Nonlinear relationships between covariates and a response variable of interest are frequently encountered in animal science research. Within statistical models, these nonlinear effects have, traditionally, been handled using a range of approaches, including transformation of the response, parametric nonlinear models based on theory or phenomenological grounds (e.g., lactation curves), or through fixed spline or polynomial terms. If it is desirable to learn the shape of the relationship from the data directly, then generalized additive models (GAMs) are an excellent alternative to these traditional approaches. GAMs extend the generalized linear model such that the linear predictor includes one or more smooth functions, parameterised using penalised splines. A wiggliness penalty on each function is used to avoid over fitting while estimating the parameters of the spline basis functions to maximise fit to the data without producing an overly complex function. Modern GAMs include automatic smoothness selection methods to find an optimal balance between fit and complexity of the estimated functions. Because GAMs learn the shapes of functions from the data, the user can avoid forcing a particular model to their data. Here, I provide a brief description of GAMs and visually illustrate how they work. I then demonstrate the utility of GAMs on three example data sets of increasing complexity, to show i) how learning from data can produce a better fit to data than that of parametric models, ii) how hierarchical GAMs can be used to estimate growth data from multiple animals in a single model, and iii) how hierarchical GAMs can be used for formal statistical inference in a designed experiment of the effects of exposure to maternal hormones on subsequent growth in Japanese quail. The examples are supported by R code that demonstrates how to fit each of the models considered, and reproduces the results of the statistical analyses reported here. Ultimately, I shown that GAMs are a modern, flexible, and highly usable statistical model that is amenable to many research problems in animal science, and deserve a place in the statistical toolbox.
keywords:
  - "Generalized additive model"
  - "penalised spline"
  - "Hierarchical model"
  - "Conditional effects"
  - "Basis function"
license: "CC BY"
format:
  elsevier-pdf:
    mathspec: true
    keep-tex: true
    mainfont: Arial
    include-in-header: 
      - preamble.tex
    number-sections: false
    journal:
      name: Animal -- Open Space
      formatting: review
      model: 1p
      layout: onecolumn
      cite-style: authoryear
execute:
  echo: false
bibliography: references.bib
csl: elsarticle.cls
crossref:
  fig-title: "Fig."
---

# Implications

Nonlinear relationships between covariates and a response variable of interest are frequently encountered in animal science research. Generalized additive models and automatic smoothness selection via penalised splines provide an attractive, flexible, data-driven statistical model that is capable of estimating these relationships. I provide a description of the generalized additive model and demonstrate its use on three typical data examples; i) a lactation curve, ii) growth curves in commercial pigs, and iii) a experiment on the effects of maternal hormones on growth rates in Japanese quail.

\newpage

# Specification table

\singlespacing
\begin{threeparttable}
\begin{tabular*}{\linewidth}{| p{0.25\linewidth} | p{0.7\linewidth} |}
\hline
Subject & Livestock farming systems \\ \hline
Specific subject area & Quantitative analysis of animal performance, growth, and modelling \\ \hline
Type of data & Table, graph, code \\ \hline
How data were acquired & Lactation curve: unstated in original source.

Pig growth data: weight measurements derived from a depth camera (iDOL65, dol-sensors a/s, Aarhus, Denmark) and a YOLO (you only look once) algorithm.

Quail growth experiment: body mass recorded using a digital balance. \\ \hline

Data format & Lactation curve: processed (averaged).

Pig growth data: processed (averages of multiple depth camera-based weight measurements).

Quail growth experiment: Raw. \\ \hline
Parameters for data collection & Lactation curve: daily fat content of milk for
a single animal (cow 7450).

Pig growth data: Pigs raised under conventional husbandry conditions at two farms.

Quail growth experiment: 158 eggs from adult Japanese quails provided by Finnish private local breeders were injected with one or a combination of maternal hormones or
a saline solution control, and those eggs that hatched successfully were
monitored for 78 days for a range of parameters including body mass. \\ \hline

Description of data collection & Lactation curve: fat content of daily milk production was measured for a single cow.

Pig growth data: a depth camera observed the pigs \emph{in situ} and a computer algorithm converted the digital imagery of individual animals into weight estimates.

Quail growth experiment: the body mass of hatched quail was recorded at 12 hours post hatching, once every three days for days three to 15, and weekly thereafter until day 78. \\ \hline
Data source location & Lactation curve: unknown.

Pig growth data: Data from two farms were reported in the original study: A commercial farm in Gronau, Germany, and the experimental farm of the Department of Animal and Veterinary Sciences, Aarhus University, Viborg, Denmark.

Quail growth experiment: Finland \\ \hline
Data accessibility & Repository name: Zenodo

Data identification number: 10.5281/zenodo.15777270

Direct URL to data: https://doi.org/10.5281/zenodo.15777270 \\ \hline
\end{tabular*}
\end{threeparttable}

\doublespacing

```{r}
#| label: setup
#| results: false
#| message: false
pkgs <- c(
  "broom",
  "readxl", "forcats", "readr", "mgcv", "gratia", "ggplot2", "dplyr", "tibble",
  "patchwork", "lubridate", "ggokabeito", "marginaleffects"
)
res <- vapply(
  pkgs, library, logical(1), character.only = TRUE,
  logical.return = TRUE
)

theme_set(theme_bw())
```

```{r}
#| label: functions
#| warning: false
wiggliness_plot <- function(scaling, eqn_size = 3) {
  # lambda <- 10
  # simulate data
  df <- tibble(x = withr::with_seed(1, runif(100))) |>
    mutate(
      f = gw_f2(x),
      f = f + min(f),
      y = withr::with_seed(1234, rnorm(100, mean = f, sd = 1))
    )
  # setup model
  m <- gam(
    y ~ s(x, bs = "bs", k = 16, pc = 0), data = df, drop.intercept = FALSE
  )
  G <- gam(
    y ~ s(x, bs = "bs", k = 16, pc = 0),
    data = df, fit = FALSE, sp = m$sp,
    control = gam.control(scalePenalty = FALSE)
  )
  G$lsp0 <- log(m$sp * scaling)
  fit <- gam(G = G, control = gam.control(scalePenalty = FALSE))
  # value of penalty
  b <- smooth_coefs(fit, select = "s(x)")
  S <- fit$smooth[[1]]$S[[1]]
  bSb <- t(b) %*% S %*% b
  bSb <- round(bSb)

  sim_evenly <- tibble(x = seq(0, 1, length = 500))

  bf_bs <- basis(fit, select = "s(x)", at = sim_evenly)

  fv <- fitted_values(fit, data = sim_evenly)

  fv |>
    ggplot(
      aes(
        x = x, y = .fitted
      )
    ) +
    geom_point(
      data = df,
      aes(
        x = x, y = y
      ),
      alpha = 0.5
    ) +
    geom_line(
      data = bf_bs,
      aes(
        x = x, y = .value, group = .bf, colour = .bf
      )
    ) +
    geom_line() +
    labs(
      x = "x", y = "y"
    ) +
    guides(colour = "none") +
    annotate(
      "text", x = 0.5, y = 9, hjust = 0,
      label = bquote(
        integral(italic(f) * "''" * (x)^2 ~ dx == .(bSb), x[1], x[n])
      ),
      size = eqn_size
    ) +
    lims(x = c(0, 1), y = c(-2.5, 10.5))
}
```

# Introduction

Many research questions in the animal sciences involve nonlinear relationships between covariates and a response variable of interest. A classic example, with a long history of statistically-based and mathematically-based research, is the plethora of models that have been described for the estimation of lactation curves from test day data or automatic milking machines [e.g., @Macciotta2011-ch]. Another, is the estimation of growth curves in the context of breeding and genetics [e.g., @White1999-jg]. Despite the frequency with which such nonlinear relationships are encountered, surprisingly little use of generalized additive models (GAMs) has been seen in animal science to date. Notable exceptions include @Hirst2002-eh, @Yano2014-cn, @Huang2023-rs, and @Benni2020-rc

In part, this lack of uptake reflects a traditional statistics workflow grounded in mixed effects modelling. Statistical training rarely includes more advanced models like GAMs, and GAMs are sufficiently different an approach that many researchers may be wary of using them because they are unfamiliar with the nomenclature used to describe the models or the software used to fit them. Where GAMs have been used in animal science, best practice is often not followed, for example in the choice of smoothness selection method, or failing to adequately specify the conditional distribution of the response or transform the response to better meet the distributional assumptions of a Gaussian model [e.g. @van-Lingen2023-sd].

More attention has been paid to the use of splines, frequently in comparisons against some form of polynomial-based model, especially Legendre polynomials [e.g., @Nagel-Alne2014-bg; @Silvestre2006-uw; @Macciotta2010-kt; @Brito2017-bx]. There, the focus has largely been on the choice of the number of knots in the spline basis expansion and on the placement of those knots. Modern GAMs largely make such choices redundant; with penalised splines, a wiggliness penalty is used to avoid over-fitting, and low-rank eigen bases, such as the low-rank thin plate regression spline basis of @Wood2003-qy avoid the knot-placement issue for most problems.

A guide to the use of GAMs in the animal science setting, written with users in mind, is needed to raise awareness of the utility of these flexible models and to promote best practice in fitting GAMs. Below, I describe GAMs and demonstrate visually how they work. Then I apply GAMs to three different examples representing typical data encountered in animal science. The examples below are supported by tutorials containing the computer code needed to fit the models in the R statistical software [@Rcore], which are available along side the source code for the manuscript itself.

# Materials and methods

## Generalized additive models

A basic GAM [@Hastie1990-bx] has the following form
\begin{linenomath*}
\begin{align*}
y_i &\sim \mathcal{D}(\mu_i, \phi) \\
\mathbb{E}(y_i) &= \mu_i \\
g(\mu_i) &= \mathbf{X}_i \boldsymbol{\gamma} + \sum_j f_j(\bullet), \; i = 1, \dots, n; \; j = 1, \dots, J
\end{align*}
\end{linenomath*}
where $y_i$ is a univariate response variable of interest that is modelled on the scale of a link function $g()$, $\mathcal{D}(\mu_i, \phi)$ is a distribution, typically from, though not limited to, the exponential family of distributions, with mean $\mu_i$ and scale parameter $\phi$. $\mathbf{X}_i$ is the $i$th row of the model matrix of any parametric terms (including the model intercept or constant term), and $\boldsymbol{\gamma}$ the associated regression parameters. The $f_j$ are $J$ smooth functions of one or more covariates; I use $\bullet$ as a placeholder for this definition, but in the simplest case of a smooth of a single continuous covariate we have $f_j(x_{ij})$, where $x_{ij}$ is a univariate covariate.

In the remainder of this section, I aim to present *just enough* detail about GAMs to afford the reader a general understanding of what is involved in estimating such a model so that they can appreciate how GAMs work, and how we aim to avoid overfitting or having to choose how complex each smooth function should be. The original approach [@Hastie1990-bx] for fitting GAMs, known as *backfitting*, required the analyst to specify how many degrees of freedom each function in the model should take *a priori*, which was viewed by many as being too subjective for more than exploratory analysis. With modern automatic smoothness selection methods, this problem has largely been resolved, through the use of penalised splines and fast algorithms for smoothness selection.

### Penalised splines {.unnumbered}

In a GAM, the smooth functions $f_j()$ are typically represented in the model using splines, although other functions fit into this framework, most notably iid Gaussian random effects. A spline is composed of $K$ basis functions, $b_k()$, and their associated coefficients, $\beta_k$
$$
f_j(x_{ij}) = \sum_{k = 1}^{K} \beta_kb_k(x_{ij}) .
$$
For identifiability reasons, the basis is subject to a sum-to-zero constraint to remove the constant function from the span of the basis, which allows a separate constant term or intercept in the model; this is desirable if we also want to include categorical (factor) terms in the model for example. [@fig-how-splines-work]a shows a B spline basis ($K$ = 16) for a covariate $x$ after the absorbing the sum-to-zero constraint into the basis.

Fitting a spline then involves finding estimates for the coefficients of basis functions, $\beta_k$. These coefficients weight the individual basis functions as shown in [@fig-how-splines-work]b. To find the value of the fitted spline at each value of $x$, we sum up the values of the weighted basis functions evaluated at each value of $x$. This yields the light blue curve in [@fig-how-splines-work]b. The coefficients for the basis functions, $\beta_k$, are determined by forcing the fitted function to go as close to the data as possible. As shown in [@fig-how-splines-work]b, we largely recover the true function from which the data were simulated.

Expanding $\mathbf{x}_{j}$ into many basis functions in this way means we must be cognizant of the risks of over fitting the sample of data we have to hand. Taken to the extreme, we could obtain an arbitrarily close fit to the data by using as many basis functions as there are data (i.e., $K = n$), but all this would achieve in practice is the replacement of the data with a set of coefficients.

```{r}
#| label: fig-how-splines-work
#| fig-cap: Illustration of how penalised splines work. A spline basis expansion (a) and associated penalty matrix $\mathbf{S}$ (c) are formed for a covariate $x$. Model fitting involves finding estimates for the coefficients of the basis functions that make the fitted spline (thick, blue curve) go as close to the data (black points) as possible, without over fitting (b). In (a) and (b) the basis functions are shown as thin coloured lines and are from a B spline basis. The sum-to-zero identifiability constraint needed so that an intercept can be included in the model has been absorbed into the basis shown. The dashed horizontal line in (b) is the estimated value of the intercept. The penalty matrix (c) encodes how wiggly each basis function is in terms of its second derivative.
#| fig-width: 10
#| fig-height: 3
# B splines
# simulate data
sim_d <- tibble(x = withr::with_seed(1234, runif(200))) |>
  mutate(
    f = gw_f2(x),
    y = withr::with_seed(1234, rnorm(200, mean = f, sd = 1))
  )

sim_evenly <- tibble(x = seq(0, 1, length = 200)) |>
  mutate(
    f = gw_f2(x),
    y = withr::with_seed(1234, rnorm(200, mean = f, sd = 1))
  )

sim_bs <- gam(
  y ~ s(x, bs = "bs", k = 16),
  data = sim_d,
  method = "REML"
)

bf_raw_bs <- basis(
  s(x, bs = "bs", k = 16),
  data = sim_d,
  at = sim_evenly,
  constraints = TRUE
)

bf_bs <- basis(sim_bs, select = "s(x)", at = sim_bs)

plt_lims <- lims(x = c(0, 1), y = c(-3.5, 11.5))

p_bs_raw <- bf_raw_bs |>
  mutate(.value = .value * 1) |>
  draw() +
  geom_point(
    data = sim_d, aes(x = x, y = y),
    alpha = 1,
    inherit.aes = FALSE,
    size = 0.5
  ) +
  geom_function(
    fun = gw_f2,
    aes(x = x),
    data = bf_raw_bs |> distinct(x, .keep_all = TRUE),
    colour = "black", linewidth = 1.5, alpha = 0.3
  ) +
  plt_lims +
  labs(title = NULL)

p_only_bs <- bf_raw_bs |>
  draw() +
  labs(title = NULL)

p_bs <- bf_bs |>
  draw() +
  geom_point(
    data = sim_d, aes(x = x, y = y),
    alpha = 1,
    inherit.aes = FALSE,
    size = 0.5
  ) +
  geom_line(
    data = data.frame(
      y = rep(model_constant(sim_bs), 200),
      x = sim_d$x
    ),
    aes(x = x, y = y, colour = NULL, group = NULL),
    linetype = "dashed"
  ) +
  geom_function(
    fun = gw_f2,
    aes(x = x),
    data = bf_raw_bs |> distinct(x, .keep_all = TRUE),
    colour = "black", linewidth = 1.5, alpha = 0.3
  ) +
  geom_line(
    data = bf_bs |>
    group_by(x) |>
    summarise(.spline = sum(.value) + model_constant(sim_bs)),
    aes(
      x = x, y = .spline, colour = NULL, group = NULL
    ),
    colour = palette.colors()[3],
    linewidth = 1.5,
    alpha = 0.6
  ) +
  plt_lims +
  labs(title = NULL)

# plot the penalty S
S_bs <- penalty(sim_bs, select = "s(x)")
p_S_bs <- S_bs |>
  draw() +
  labs(title = NULL, caption = NULL) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# not decided yet if I want the raw, unweighted basis on the data and true
# function plot (c)
# p_only_bs + p_S_bs + p_bs_raw + p_bs + plot_layout(ncol = 2, nrow = 2) +
#  plot_annotation(tag_levels = "a", tag_suffix = ")")
# note I reordered the 1 row, 3 col version below
p_only_bs + p_bs + p_S_bs + plot_layout(ncol = 3, nrow = 1) +
  plot_annotation(tag_levels = "a", tag_suffix = ")")
```

This raises the question of how many basis functions should be used? One option, if there are $n$ unique values of the covariate $x$, is to use $n$ basis functions, leading to a situation where our model would have as many coefficients as data. However, we do not gain anything by using $n$ basis functions from a statistical viewpoint. In practice we typically use $\mathcal{N} \ll n$ basis functions. Yet, even with just $\mathcal{N}$ basis functions, we face the very real situation that our model may overfit the data if $\mathcal{N}$ is too large. To address this, modern approaches to fitting GAMs use penalised splines.

If our aim is to avoid overfitting, we need to penalise highly complex fitted functions, and hence need to define what we mean by "complex". A complex function would be one that "wiggles" about markedly as we move from low to high values of the covariate, $x$. Such wiggling around implies that the function has a high amount of curvature, which we measure using the second derivative of the fitted function. While there are other definitions for complexity that we use, this the typical measure used for splines in statistical models. Therefore, to measure the wiggliness of a fitted function we need to integrate, or sum up, the second derivative of the function $f$ over the range of $x$
$$
\int_{x} f^{\prime\prime}(x)^2 \, dx = \boldsymbol{\beta}^{\mathsf{T}} \mathbf{S} \boldsymbol{\beta}
$$ {#eq-wiggliness}
where $f^{\prime\prime}$ indicates the second derivative of $f$, and note that we are integrating the *square* of the second derivative because we need to allow for both negative and positive curvature as the function wiggles. Conveniently, we can compute this integral as a function of the model coefficients $\boldsymbol{\beta}$ as shown on the right hand side of [@eq-wiggliness]. $\mathbf{S}$ is a known penalty matrix, which encodes the complexity of the basis functions. The penalty matrix for the basis expansion shown in [@fig-how-splines-work]a is displayed in [@fig-how-splines-work]c.

@fig-wiggliness shows three different estimated smooths for the simulated shown in @fig-how-splines-work and their wiggliness or integrated squared second derivative. Fitting a GAM requires us to balance the fit to the data *and* the complexity of the resulting model; we wish to avoid both over fitting the data ([@fig-wiggliness]a) and over smoothing ([@fig-wiggliness]c) the data. We want the fitted functions to be just "wiggly enough" to approximate the true, but unknown relationships ([@fig-wiggliness]b).

```{r}
#| label: fig-wiggliness
#| fig-cap: Illustration of how the wiggliness penalty controls the resulting fit of a penalised spline. The weighted basis functions are shown as thin coloured lines. In each panel a penalised spline is shown by the solid black line, which has been fitted to the data points shown. The wiggliness value of the spline, the integrated squared derivative of the fitted spline over $x$ is given in the upper right of each panel. The spline in (a) is over fitted to the data, resulting in a very wiggly function with a large wiggliness value. The spline in (c) is over smoothed, resulting in a a simple fitted function with low wiggliness, but which does not fit the data well. The spline in (b) represents a balance between fit to the data and complexity of fitted function. The smoothing parameter for the spline, $\lambda$, is used as a tuning parameter in the model, which ultimately controls this balance between fit and complexity.
#| warning: false
#| fig-width: 10
#| fig-height: 3
plt_a <- wiggliness_plot(scaling = 0.0000005)
plt_b <- wiggliness_plot(scaling = 0.00002)
plt_c <- wiggliness_plot(scaling = 0.005)

plt_a + plt_b + plt_c + 
  plot_annotation(
    tag_levels = "a", tag_suffix = ")"
  )
#
```

We find estimates of the basis function coefficients, $\beta_k$, that make the fitted spline go as close to the data as possible; in practice, this is done by maximising the penalised log likelihood
\begin{linenomath*}
\begin{equation*}
\ell_p(\boldsymbol{\beta}) = \ell(\boldsymbol{\beta}) - \frac{1}{2 \phi} \sum_{j} \lambda_j \boldsymbol{\beta}_j^{\mathsf{T}} \mathbf{S}_j \boldsymbol{\beta}_j \; ,
\end{equation*}
\end{linenomath*}
where $\ell_p(\boldsymbol{\beta})$ is the penalised log likelihood and $\ell(\boldsymbol{\beta})$ the log likelihood of the data, given the $\boldsymbol{\beta}$, and the remainder is the wiggliness penalty for the model. The log likelihood ($\ell(\boldsymbol{\beta})$) measures how well our model fits the data, while the wiggliness penalty $\boldsymbol{\beta}_j^{\mathsf{T}} \mathbf{S}_j \boldsymbol{\beta}_j$ measures how complex the model is. Note that any parametric coefficients, $\boldsymbol{\gamma}$, including the intercept have been absorbed into $\boldsymbol{\beta}$ for convenience. The $\lambda_j$ are known as the smoothing parameters of the model, and it is these smoothing parameters that actually control how much the wiggliness penalty affects the $\ell_p(\boldsymbol{\beta})$. We can think of the $\lambda_j$ as tuning or hyper-parameters of the model.

As mentioned in the introduction, the knot placement problem can largely be averted through the use of a low rank thin plate regression spline basis [@Wood2003-qy]. The spline basis used in the above description was a B spline basis, and for that basis the knots were placed at evenly-spaced quantiles of the covariate. In general, the exact positioning of the knots is unimportant, so long as they are spread out over the range of the covariate. If we wish to avoid the knot placement altogether, we could replace the B spline basis with a low rank thin plate regression spline basis (TPRS). We start with a radial basis function at each unique value of the covariate. This full, rich basis is then transformed and truncated through an eigen decomposition to retain the $k$ eigenvectors with the smallest eigenvalues. The decomposition removes much of the excessive wiggliness that $n$ basis functions would provide, while retaining many of the good properties of the original basis [@Wood2003-qy]. The main downside of the TPRS basis is that it is computationally expensive to form the basis when setting up the model; for larger data problems, with many thousands of data, simpler bases such as the cubic regression spline or B spline may be preferred. In the *mgcv* software used here, the TPRS basis is the default basis used for univariate smooths [@Wood2025-zt, @Wood2011-kn].

The algorithms used to fit GAMs need to estimate the model coefficients, $\boldsymbol{\beta}$, and choose appropriate values of the smoothing parameters, $\lambda_j$. This process is known as smoothness selection, and there are several approaches to smoothness selection that can be taken. One is treat the problem as one of prediction, and choose $\lambda_j$ in such a way as to minimise the cross-validated prediction error of the model. In practice it would be computationally costly to actually cross-validate the model for fitting, so we approximate the prediction error by minimising the generalised cross validation (GCV) error, AIC, or similar measure. An alternative means of smoothness selection is to take a Bayesian view of the smoothing process [see @Miller2025-bd for an accessible introduction to this viewpoint]; in doing so, the $\boldsymbol{\beta}_j^{\mathsf{T}} \mathbf{S}_j \boldsymbol{\beta}_j$ should be viewed as multivariate normal priors on the $\boldsymbol{\beta}$. From this Bayesian view of smoothing, we find that the criterion we wish to minimise is that of a mixed effects model. Hence, we can think of the wiggly parts of the $f_j$ as being fancy random effects, while the smooth parts of the $f_j$ are fixed effects, and the $\lambda_j$ are inversely proportional to the random effect variances one would observe if the model were fitted as a mixed effects model. The Bayesian approach to smoothing can involve fully bayesian estimation using Markov chain Monte Carlo (MCMC) or simulation free estimation via the integrated nested Laplace approximation (INLA), or, using the equivalence of splines and random effects, we can take an empirical bayesian approach, which yields the posterior modes  of the $\boldsymbol{\beta}_j$, or the maximum a posteriori (MAP) estimates.

## Examples {.unnumbered}

In the remainder of the section I describe three representative examples that demonstrate the utility of GAMs to address problems in animal science.

### Lactation curves {.unnumbered}

As a simple illustration of the benefits of GAMs to learn the functional form of a relationship between a response variable and a covariate, rather then impose one through a parametric model, I reanalyse a small data set of average daily fat content per week of milk from a single cow ([@fig-lactation-curves]). The data were reported in @Henderson1990-bd. Initially, I followed their [@Henderson1990-bd] analysis and fitted a Gamma generalized linear model (GLM) with log link function. However, subsequent analysis of this model (and the GAM alternative described below) showed that the average daily fat data are under dispersed relative to the assumed conditional distribution. Instead, a Tweedie GLM (log link) was fitted, which has the same response shape as the gamma GLM but is more easily compared with the Tweedie GAM described below. The Tweedie GLM fitted had the following form
\begin{linenomath*}
\begin{align*}
\mathtt{fat}_i & \sim \mathcal{T}(\mu_i, \phi) \\
\log(\mu_i)  & = \beta_0 + \beta_1 \log(\mathtt{week}_i) + \beta_2 \mathtt{week}_i \;.
\end{align*}
\end{linenomath*}
@Henderson1990-bd compared the fit of the GLM model with several other formulations and models, including the model of @Wood1967-re
\begin{linenomath*}
\begin{equation*}
\mathtt{fat}_i = \alpha \, \mathtt{week}_i^{\delta} \exp(\kappa \, \mathtt{week}_i) + \varepsilon
\end{equation*}
\end{linenomath*}
where $\alpha$, $\delta$, and $\kappa$ are parameters whose values are to be estimated, and $\varepsilon$ is a Gaussian error term. The linear predictors in the GLM and Wood's model are equivalent because of the log link function; $\beta_0 = \log{\alpha}$, $\beta_1 = \delta$, and $\beta_2 = \kappa$. However, we would not expect both models to produce the same fitted lactation curve as different distributional assumptions are being made; in the GLM version we assume the average daily fat values are conditionally Tweedie distributed, while in Wood's model they are assumed to be conditionally Gaussian distributed.

These models were compared with a GAM version of the log-link, Tweedie GLM, where the fixed functional form for the lactation curve has been replaced by a smooth function to be estimated from the data
\begin{linenomath*}
\begin{align*}
\mathtt{fat}_i & \sim \mathcal{T}(\mu_i, \phi) \\
\log(\mu_i)  & = \beta_0 + f(\mathtt{week}_i)
\end{align*}
\end{linenomath*}

Wood's model [@Wood1967-re] was estimated via a nonlinear least squares model using the `nls()` function in R [version 4.5.0, @Rcore]. The Tweedie GLM and GAM were fitted using the `gam()` function of the *mgcv* package [version 1.9.3, @Wood2025-zt, @Wood2011-kn] for R. $k = 9$ basis functions were used for $f(\mathtt{week}_i)$ after application of the identifiability constraint.

### Pig growth {.unnumbered}

In this second example, I illustrate how individual growth curves can be estimated using a hierarchical GAM. A hierarchical GAM [HGAM, @Pedersen2019-ff] is a GAM that includes smooths are different hierarchical levels of the data structure; HGAMs are the equivalent of hierarchical or mixed effects models. In this example, different parameterisations of the model lead to either direct estimation of each individual animals growth curve, or a decomposition into an average growth curve plus animal-specific deviations from this average curve. Although previously covered by @Pedersen2019-ff in detail, a new addition here is the use of a constrained factor smooth to produce animal-specific deviations that are truly orthogonal to the average curve. This basis type was not available to @Pedersen2019-ff, and helps avoid switching to a first derivative penalty for the animal-specific curves that would be required to make the model identifiable if a "factor by" smooth was used.

Automated estimation of body weights is a useful on-farm method for continuously monitoring growth of commercial pigs. I analyse a subset of the weight data reported by @Franchi2023-xq from a study by @Bus2025-gg, using data from a single pen of 18 pigs (@fig-pig-fitted-plot). The body weight data were obtained using a depth camera (iDOL65, dol-sensors A/S, Aarhus, Denmark), and the each weight observation is the daily average of multiple measurements made by the camera. As the number of weight measurements made each day varied per animal and per day, each weight observation in the data is the average of a variable number of measurements. To accomodata this, the model included the number of measurements averages as an observation weight, where the precision of each response data is proportional to the number of measurements averaged.

The data exhibit common and animal-specific variation. To model these features, four different GAMs were fitted to the pig weight data. All of the models ultimately provide animal-specific estimates of weight over time, but they decompose the growth curves in different ways. The weight data were assumed to be conditionally gamma distributed, with log link, $\mathtt{weight}_i \sim \mathcal{G}(\mu_i, \phi)$, with $\log(\mu_i) = \eta_i$, where $\boldsymbol{\eta}$ is the linear predictor. The linear predictors for each of the four models were:
\begin{linenomath*}
\begin{align*}
\text{P1:} \; \eta_i & = \beta_{a(i)} + f_{a(i)}(\mathtt{day}_i) \\
\text{P2:} \; \eta_i & = \beta_0 + f_1(\mathtt{day}_i) + f_{a(i)}(\mathtt{day}_i) \\
\text{P3:} \; \eta_i & = \beta_0 + f^{\ast}_{a(i)}(\mathtt{day}_i) \\
\text{P4:} \; \eta_i & = \beta_0 + f_1(\mathtt{day}_i) + f^{\ast}_{a(i)}(\mathtt{day}_i)
\end{align*}
\end{linenomath*}
where $a(i)$ indicates to which animal the $i$th observation belongs. Model P1 includes the mean weight of each animal through a parametric factor term, $\beta_{a(i)}$, plus a smooth of observation day $\mathtt{day}_i$ *per* animal. The parametric factor term is required because the animal-specific smooths in this model are each subject to the sum-to-zero-constraint and as such do not contain the constant functions that are needed to model average weight of each animal. These smooths are known informally as "factor by smooths". Model P2 decomposes the data into an average growth curve, $f_1(\mathtt{day}_i)$, plus smooths, one per animal, that represent deviations from the average smooth, $f_{a(i)}(\mathtt{day}_i)$. Unlike the *factor by smooths*, these deviation smooths do include constant terms for each animal's average weight, hence only a constant term, $\beta_0$ is include in the parametric part of this model. In both models P1 and P2, the $f_{a(i)}(\mathtt{day}_i)$ smooths each have their own smoothing parameters, allowing the wiggliness of each animal's growth curve to vary, if supported by the data.

Model P3 is similar to model P1, except that each animal's growth curve, $f^{\ast}_{a(i)}(\mathtt{day}_i)$, shares a single smoothing parameter for the wiggliness and hence assumes that the wiggliness of each curve is similar. These smooths are denoted with a superscript $\ast$ to indicate the shared smoothing parameter, and can be thought of as the smooth equivalent of random slopes and intercepts; informally, we refer to these as *random smooths*. These smooths are fully penalised and contain constant terms to model the average weight of each animal, hence only the intercept, $\beta0$ is included in the parametric part of the model. Model P4 is similar to model P2 in terms of the decomposition into an *average* smooth and animal-specific smooth deviations, but, like model P3, the animal-specific smooths share a smoothing parameter and therefore assume they have similar wiggliness.

For further details on the different approaches used in the models described above, see @Pedersen2019-ff. Each of the smooths in the models used $k = 9$ basis functions, after application of identifiability constraints.

### Japanese quail {.unnumbered}

In the third example, I demonstrate how to fit GAMs in the context of a designed experiment to obtain estimated treatment effects.

The data [@Sarraude2020-cw] are from an experiment into the short- and long-term effects on quail of elevated exposure to different types of maternal thyroid hormone in Japanese quail, *Coturnix japonica* [@Sarraude2020-fu]. Briefly, the yolks of $n = \text{57}$ eggs were experimentally manipulated by injection with the prohormone, T~4~, its active metabolite, T~3~, or both T~4~ and T~3~ (T~3~T~4~), or a saline solution that acted as a control (CO). Body mass was initially measured 12 hours after hatching. Between days 3--15, body mass was  recorded every three days, then, between days 15--78, once per week using a digital balance.

The quail weight data were provisionally assumed to be conditionally gamma distributed. Despite this being a reasonable working assumption, model diagnostics identified deviations from this assumption, and instead the data were modelled as being conditionally Tweedie distributed. The Tweedie family of distributions contains the Poisson (power, $p = 1$) and gamma distributions ($p = 2$) as special cases, and is more flexible than gamma, and allows for a range of mean-variance relationships through the power parameter, $p$, of the distribution. In the Tweedie GAMs that were fitted, the power parameter $p$ was allowed to vary between 1--2 and was estimated as an additional model constant term. Models fitted were:
\begin{linenomath*}
\begin{align*}
\text{Q1:} \; \eta_i & = \beta_0 + f(\mathtt{day}_i) + f_{\text{sex}(i)}(\mathtt{day}_i) + f^{\ast}_{\text{egg}(i)}(\mathtt{day}_i) + \xi_{\text{mother}(i)} \\
\text{Q2:} \; \eta_i & = \beta_0 + f(\mathtt{day}_i) + f_{\text{treat}(i)}(\mathtt{day}_i) + f_{\text{sex}(i)}(\mathtt{day}_i) + f^{\ast}_{\text{egg}(i)}(\mathtt{day}_i) + \xi_{\text{mother}(i)} \\
\text{Q3:} \; \eta_i & = \beta_0 + f(\mathtt{day}_i) + f_{\text{treat}(i)}(\mathtt{day}_i) + f_{\text{sex}(i)}(\mathtt{day}_i) + f_{\text{treat}(i),\text{sex}(i)}(\mathtt{day}_i) \\
                     & + f^{\ast}_{\text{egg}(i)}(\mathtt{day}_i) + \xi_{\text{mother}(i)} \\
\text{Q4:} \; \eta_i & = \beta_0 + f(\mathtt{day}_i) + f_{\text{treat}(i)}(\mathtt{day}_i) + f_{\text{sex}(i)}(\mathtt{day}_i) + f_{\text{treat}(i),\text{sex}(i)}(\mathtt{day}_i) \\
                     & + \psi_{\text{egg}(i)} + \xi_{\text{mother}(i)}
\end{align*}
\end{linenomath*}
where $\mathtt{day}_i$ is the number of days since hatching, $\text{treat}(i)$, $\text{sex}(i)$, and $\text{egg}(i)$ indicate to which treatment group, sex, and bird the $i$th observation belongs. Smooth functions with subscript $\text{treat}(i)$, $\text{sex}(i)$, or $\text{egg}(i)$ are factor-smooth interactions representing deviations from the "average" smooth, $f(\mathtt{day}_i)$, for the indicated factor. $f_{\text{treat}(i),\text{sex}(i)}(\mathtt{day}_i)$ represents a higher order factor-smooth interaction, which allows the time varying treatment effects to also vary between male or female quail. $\psi_{\text{egg}(i)}$ and $\xi_{\text{mother}(i)}$ are iid Gaussian random intercepts for individual quail and their mother respectively. A $f^{\ast}$ represents a random smooth, where each smooth in the set shares a smoothing parameter. The factor-smooth interactions in this model were fitted using the constrained factor-smooth interaction basis in the *mgcv* package; this basis excludes the main effects (and lower-order terms in the case if higher-order interactions) to insure that the basis functions are orthogonal to those lower order terms.

Model Q1 represents a null model containing no treatment effects but models the remaining features of the data, decomposing the growth curves into an average effect, a sex-specific effect, and individual quail-specific effects, plus a maternal effect. Model Q2 extends Q1 by adding the treatment effect, $f_{\text{treat}(i)}(\mathtt{day}_i)$, which models deviations from the average curve for each of the four treatment levels. Model Q3 further extends the Q2 to allow different treatment-specific curves for male and female quail. Model Q4 is a variant of Q3, replacing the quail-specific growth curve deviations with an individual level random intercept. *A priori*, model Q3 represents the complete set of hypotheses an analyst might expect to consider for these data.

Each smooth in the quail models used $k = 9$ basis functions after application of identifiability constraints, except the quail-specific smooths, $f^{\ast}_{\text{egg}(i)}(\mathtt{day}_i)$, which used $k = 6$ basis functions per individual. This reduced number of basis functions per smooth was a reflection that after accounting for the average shape of the growth curves, plus sex and treatment deviations, any remaining individual-specific variation from these other effects would be smaller in magnitude and less complex (wiggly).

### Smoothness selection & inference {.unnumbered}

Restricted marginal likelihood (REML) smoothness selection [@Wood2011-kn] was used to estimate each of the GAMs fitted to the example data sets. REML was used because it has slightly better performance in terms of estimating smoothing parameters compared to marginal likelihood (ML) smoothness selection. We did not use GCV smoothness selection for these examples because i) prediction error is a less important consideration here where we are interested in estimation of effects and statistical inference, and ii) GCV is prone to under smoothing [@Reiss2009-fk; @Wood2011-kn].

In the lactation curve example I use standard model appraisal teachniques to identify the most useful model. In the pig growth example, AIC was used to identify which of the decompositions of time provided the best fit, but the specific choice of model was made on other grounds, using domain knowledge.

In the quail hormone example, AIC values are reported, but *a priori* I chose to report results from model Q3, the "full" model from the point of view of the potential hypotheses under consideration; as the treatment effect was allowed to vary between sexes, I take an estimation-based approach and quantify the magnitudes of any treatment by sex interactions using the most complex model. The remaining models are fitted and reported on briefly for illustrative purposes; performing model term selection (for example, testing the higher order $f_{\text{treat}(i)}(\mathtt{day}_i)$ interaction, and deciding on the basis of a *p* value or AIC whether to retain it in the model or not), would introduce biases into the inference process. As we currently lack good post selection inference methods for handling this kind of selection, it is prudent to simply not enter into such selection procedures.

Using model Q3 allows there to be treatment differences between male and female quail; subsequent estimation of values of interest and comparison among these estimates is instead the inference route followed for the quail example. To illustrate, I estimate two quantities of interest: i) the estimated growth rate (slope) of an average quail at $\mathtt{day} = \text{20}$ in all combinations of treatment and sex, and ii) the estimated weight of an average quail at the end of the experiment ($\mathtt{day} = \text{78}$), again for all combaination of treatment and sex. All pairwise comparisons among treatment levels within sex were performed, with adjustment of $p$ values to control the false discovery rate using the Benjamini--Yekutieli [@Benjamini2001-kh] procedure. Estimates and pairwise comparisons were computed using the `slopes()` and `predictions()` functions of the *marginaleffects* package for R [version 0.27.0, @Arel-Bundock2024-nw]. Estimates of the expected growth curves for average quail in all combinations of treatment and sex were produced using the `conditional_values()` function in the R package *gratia* [version `r packageVersion("gratia")`, @Simpson2024-ml].

All figures were produced using the R packages *gratia* and *ggplot2* [version 3.5.2, @Wickham2016-dg].

# Results

## Lactation curves

```{r}
#| label: lactation-example-code
lactation <- data.frame(
  yield = c(0.31, 0.39, 0.50, 0.58, 0.59, 0.64, 0.68, 0.66,
            0.67, 0.70, 0.72, 0.68, 0.65, 0.64, 0.57, 0.48,
            0.46, 0.45, 0.31, 0.33, 0.36, 0.30, 0.26, 0.34,
            0.29, 0.31, 0.29, 0.20, 0.15, 0.18, 0.11, 0.07,
            0.06, 0.01, 0.01),
  week = seq_len(35)
)

# GLM fit
yield_glm <- gam(
  yield ~ log(week) + week,
  data = lactation,
  family = tw(link = "log"), #Gamma("log"),
  method = "REML"
)

# NLS fit
yield_nls <- nls(
  yield ~ a * week^b * exp(c * week),
  data = lactation,
  start = list(a = 1, b = 1, c = .01)
)

# Gamma GAM
yield_gam <- gam(
  yield ~ s(week),
  data = lactation,
  method = "REML",
  family = Gamma(link = "log")
)

yield_tw <- gam(
  yield ~ s(week),
  data = lactation,
  method = "REML",
  family = tw(link = "log")
)

# The Gamma has the wrong mean variance relationship & worse fit that Tweedie,
# use Tweedie
yield_gam <- yield_tw

n_new <- 200
ds <- yield_gam |>
  data_slice(
    week = evenly(week, n = n_new)
  )

set.seed(1)
yield_nls_boot <- nlstools::nlsBoot(yield_nls, niter = 999)
fv_nls <- nlstools::nlsBootPredict(
  yield_nls_boot,
  newdata = ds,
  interval = "confidence"
) |>
  as_tibble() |>
  rename(
    .fitted = "Median",
    .lower_ci = "2.5%",
    .upper_ci = "97.5%"
  ) |>
  bind_cols(ds) |>
  mutate(
    .row = row_number(),
    .se = rep(NA, length = n())
  ) |>
  relocate(
    .row,
    .before = 1
  ) |>
  relocate(
    week,
    .after = .row
  ) |>
  relocate(
    .se,
    .after = .fitted
  )

fv_glm <- yield_glm |>
  fitted_values(
    data = ds
  )

fv_gam <- yield_gam |>
  fitted_values(
    data = ds
  )

fv_tw <- yield_tw |>
  fitted_values(
    data = ds
  )

fv_models <- fv_glm |>
  bind_rows(
    fv_nls, fv_gam
  ) |>
  mutate(
    .model = rep(c("GLM", "Wood", "GAM"), each = n_new)
  ) |>
  relocate(
    .model, .after = .row
  )

```

[@fig-lactation-curves]a shows the three lactation curves estimated using Wood's model, a Tweedie GLM, and a Tweedie GAM. While all three models capture the general shape of the lactation data, the Tweedie GLM and, to a lesser extent, Wood's model, overestimate the peak yield, with the data exhibiting a broader period of peak fat content than is captured by either model. Wood's model and the Tweedie GLM also fail to capture the features of the mid--late lactation decline in fat content, other than the general decline itself. Conversely, the GAM, as anticipated, estimates a lactation curve that more faithfully tracks the observed data. The model response residuals ($y_i - \hat{y}_i$) for Wood's model ([@fig-lactation-curves]b) and the Tweedie GLM ([@fig-lactation-curves]c) show a significant amount of unmodelled signal, while the response residuals for the Tweedie GAM ([@fig-lactation-curves]d) are much smaller and do not show a residual pattern. Despite using roughly twice as many degrees of freedom as the other models (@tbl-yield), the Tweedie GAM was clearly favoured in terms of AIC and root mean squared error of the fitted values.

```{r}
#| label: fig-lactation-curves
#| fig-cap: Results of model fitting to the average daily fat content data from @Henderson1990-bd. a) observed average daily fat content (points) and estimated lactation curves from Wood's [-@Wood1967-re] model, a Tweedie GLM, and a Tweedie GAM (lines) with associated 95% confidence (Wood's model) or 95% credible intervals (GLM and GAM). Response residuals for Wood's model (b), Tweedie GLM (c), and Tweedie GAM (d), plus scatter plot smoothers (lines) and 95% credible intervals (shaded ribbons).
#| fig-width: 10
#| fig-height: 6
yield_plt <- lactation |>
  ggplot(
    aes(
      x = week,
      y = yield
    )
  ) +
  geom_ribbon(
    data = fv_models,
    aes(
      x = week,
      ymin = .lower_ci,
      ymax = .upper_ci,
      group = .model,
      fill = .model
    ),
    alpha = 0.2, inherit.aes = FALSE
  ) +
  geom_line(
    data = fv_models,
    aes(
      x = week,
      y = .fitted,
      group = .model,
      colour = .model
    ),
    linewidth = 1
  ) +
  geom_point() +
  labs(
    x = "Week",
    y = expression(Average ~ daily ~ fat ~ (kg ~ day^{-1})),
    colour = "Model", fill = "Model"
  ) +
  scale_color_okabe_ito() +
  scale_fill_okabe_ito()

nls_resid_plt <- tibble(
  .resid = resid(yield_nls, type = "response"),
  .fitted = fitted(yield_nls)
) |>
ggplot(
  aes(
    x = .fitted,
    y = .resid
  )
) +
  geom_point() +
  geom_smooth(
    method = "gam",
    colour = palette.colors()[4],
    fill = palette.colors()[4],
    formula = y ~ s(x, bs = "cr")
) +
labs(
  x = expression(hat(y)),
  y = expression(y[i] - hat(y)[i])
)

glm_resid_plt <- tibble(
  .resid = resid(yield_glm, type = "response"),
  .fitted = predict(yield_glm, type = "response")
) |>
ggplot(
  aes(
    x = .fitted,
    y = .resid
  )
) +
  geom_point() +
  geom_smooth(
    method = "gam",
    colour = palette.colors()[3],
    fill = palette.colors()[3],
    formula = y ~ s(x, bs = "cr")
) +
labs(
  x = expression(hat(y)),
  y = expression(y[i] - hat(y)[i])
)

gam_resid_plt <- tibble(
  .resid = resid(yield_gam, type = "response"),
  .fitted = predict(yield_glm, type = "response")
) |>
ggplot(
  aes(
    x = .fitted,
    y = .resid
  )
) +
  geom_point() +
  geom_smooth(
    method = "gam",
    colour = palette.colors()[2],
    fill = palette.colors()[2],
    formula = y ~ s(x, bs = "cr")
) +
labs(
  x = expression(hat(y)),
  y = expression(y[i] - hat(y)[i])
)

design <- "
AB#
CDE"
yield_plt + guide_area() + nls_resid_plt + glm_resid_plt + gam_resid_plt +
  plot_layout(design = design, guides = "collect") +
  plot_annotation(tag_levels = "a", tag_suffix = ")")
```

```{r}
#| label: tbl-yield
#| tbl-cap: Comparison of models fitted to the lactation curve data set. Degrees of freedom and effective degrees of freedom are shown for the Wood model and Tweedie GLM, and for the Tweedie GAM, respectively, showing the complexity in terms of (effective) numbers of parameters in each model. Akaike's An information criterion (AIC) values are shown for each model, alongside an estimate of the root mean squared error of the model fit estimated from the response residuals of each model.
rmse <- function(model) {
  sqrt(sum(resid(model, type = "response")^2))
}

yield_tab <- tibble(
  Model = c("Wood", "Tweedie GLM", "Tweedie GAM"),
  "(E)DF" = c(
    attr(logLik(yield_nls), "df") |> round() |> as.character(),
    attr(logLik(yield_glm), "df") |> round() |> as.character(),
    edf(yield_gam) |> pull(.edf) |> round(3) |> as.character()
  ),
  AIC = round(AIC(yield_nls, yield_glm, yield_gam)$AIC, 2),
  RMSE = c(
    rmse(yield_nls) |> round(2),
    rmse(yield_glm) |> round(2),
    rmse(yield_gam) |> round(2)
  )
)

knitr::kable(yield_tab, caption = NA)
```

## Pig growth

```{r}
#| label: fig-pig-data-plot
#| fig-cap: todo
#| fig-width: 8
#| fig-height: 6
pw <- read_csv(
    "data/pig-weight-data.csv",
    col_types = "Ddddc"
  ) |>
  mutate(
    day = yday(date),
    animal = factor(animal, levels = seq_len(18)),
  )

pw_plt <- pw |>
  ggplot(
    aes(
      x = date,
      y = weight_estimate,
      group = animal,
      # colour = animal
    )
  ) +
  geom_point(
    alpha = 0.7,
    size = 0.7, 
    colour = "#56B4E9"
  ) +
  #geom_line() +
  guides(colour = "none") +
  labs(
    x = NULL,
    y = expression(Weight ~ (kg))
  ) +
  facet_wrap(~ animal) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#| label: pig-growth-model-fits
#| warning: false
#| cache: true
ctrl <- gam.control(nthreads = 8, trace = FALSE)

pw_m1 <- gam(
  weight_estimate ~
    animal +
    s(day, by = animal),
    family = Gamma(link = "log"),
    data = pw,
    method = "REML",
    control = ctrl,
    weights = weight_count / mean(weight_count)
  )

pw_m2 <- gam(
  weight_estimate ~
    s(day) +
    s(day, animal, bs = "sz"),
    family = Gamma(link = "log"),
    data = pw,
    method = "REML",
    control = ctrl,
    weights = weight_count / mean(weight_count)
)

pw_m3 <- gam(
  weight_estimate ~
    s(day, animal, bs = "fs"),
    family = Gamma(link = "log"),
    data = pw,
    method = "REML",
    control = ctrl,
    weights = weight_count / mean(weight_count)
)

pw_m4 <- gam(
  weight_estimate ~
    s(day) +
    s(day, animal, bs = "fs"),
    family = Gamma(link = "log"),
    data = pw,
    method = "REML",
    control = ctrl,
    weights = weight_count / mean(weight_count)
)

# chosen model - this is parsimonious in the sense allowing each pig to have its
# own wiggliness
pw_chosen <- pw_m2
```

The estimated growth curves for the 18 pigs in the pig growth example are shown in @fig-pig-fitted-plot, which were produced from model P2. Of the four models fitted, the two models that decomposed the growth effects into an average curve plus animal-specific deviations from the average curve, models P2 and P4, resulted in the most parsimonious fits from the point of view of AIC [@tbl-pig-weight-summary]. This is due to these two model forms using many fewer degrees of freedom (~64) than either of models P1 (EDF = `r model_edf(pw_m1) |> pull(.edf) |> round(2)`) or P3 (EDF = `r model_edf(pw_m3) |> pull(.edf) |> round(2)`). Models P1 and P3 do not include the average curve and as a result expend many more degrees of freedom modelling the same general shape for each animal.  Interestingly, for these data at least, allowing for a different smoothing parameter (P2) for each animals' deviation smooth seems to be preferred over using a single smoothing parameter (P4). In part, this is due to the somewhat idiosyncratic nature of each animal's growth curve in this data set.

Although the deviance explained is very high (~96%) for all models, much of this is due to use of random effects to model differences between animals, and should not be taken as a sign that the model can effectively perfectly predict the weight of a pig under similar conditions; it is clear from @fig-pig-fitted-plot that there is much unmodelled variation aorund the estimated growth curves. One feature of the data that I do not address here is the clear variation among animals in the variance of the depth camera-based weight measurements; animals 5, 6, and 9 in particular, exhibit substantially greater variation than the other animals in the data set.

@tbl-pig-weight-model-terms provides an overview of the two terms in model P2. Although the average curve ($f(\mathtt{day}_i)$) was allowed to use $k = 9$ basis functions, the wiggliness penalty has shrunk this back to `r pw_chosen |> edf(select = "s(day)") |> pull(.edf) |> round(3)` effective degrees of freedom (EDF). The animal-specific deviation smooths, $f_{\text{a}(i)}(\mathtt{day}_i)$, were fully penalised and as such used $k = 10$ basis functions per animal. Here we clearly see the effect of the wiggliness penalty, which has resulted in a reduction from a potential 180 EDF for the set of animal-specific smooths to `r pw_chosen |> edf(select = "s(day,animal)") |> pull(.edf) |> round(3)` EDF. The test of the null hypothesis for the average growth curve and the omnibus test for the pig-specific deviation smooths indicate both are statistically interesting.

```{r}
#| label: fig-pig-fitted-plot
#| fig-cap: Depth camera-based weight estimates from 18 commerical pigs. The data (black points) are the average of multiple measurements taken of each animal per day, 1 panel per pig. The panel labels indicate to the pig shown. The estimated growth curve for each pig obtained using generalized additive model P2 is shown by the blue line in each panel. The blue shaded ribbon is the 95% bayesian credible interval around the estimate curve.
#| fig-width: 8
#| fig-height: 6
pw_ds <- pw |>
  select(animal, date) |>
  data_slice(
    animal = evenly(animal),
    date = evenly(date, by = 1) |> as.Date()
  ) |>
  mutate(
    day = yday(date)
  )

fv_pw_chosen <- pw_chosen |>
  fitted_values(
    data = pw_ds
  )

fv_pw_chosen |>
  ggplot(
    aes(
      x = date,
      y = .fitted,
      group = animal
    )
  ) +
  geom_point(
    data = pw,
    aes(x = date, y = weight_estimate),
    size = 0.8,
    colour = "black"
  ) +
  geom_ribbon(
    aes(ymin = .lower_ci, ymax = .upper_ci),
    alpha = 0.2,
    fill = palette.colors()[3]
  ) +
  geom_line(colour = palette.colors()[3]) +
  facet_wrap(~ animal) +
  labs(
    x = NULL,
    y = expression(Weight ~ (kg))
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#| label: tbl-pig-weight-summary
#| tbl-cap: Comparison of models fitted to the pig weight data set. The model label is shown (see text for the specific model formulations). Effective degrees of freedom represents model complexity in terms of the (effective) number of parameters in each model. Akaike's An information criterion (AIC) values, model deviance, and deviance explained as a proportion are also reported.
glance(pw_m1) |>
  bind_rows(
    glance(pw_m2),
    glance(pw_m3),
    glance(pw_m4)
  ) |>
  select(
    c("df", "AIC", "deviance")
  ) |>
  add_column(
    "Model" = paste0("P", 1:4), .before = 1L
  ) |>
  add_column(
    "Deviance expl." = c(
      1 - (deviance(pw_m1) / null_deviance(pw_m1)),
      1 - (deviance(pw_m2) / null_deviance(pw_m2)),
      1 - (deviance(pw_m3) / null_deviance(pw_m3)),
      1 - (deviance(pw_m4) / null_deviance(pw_m4))
    )
  ) |>
  rename(
    EDF = df,
    Deviance = deviance
  ) |>
  knitr::kable(caption = NA, digits = 3)
```

```{r}
#| label: tbl-pig-weight-model-terms
#| tbl-cap: Model summary for model P2 fitted to the pig growth data set. The model term for comparison with the descriptions in the text and the label reported by the software are both shown for clarity. $k$ is the number of basis functions *per smooth*, EDF is the effective degrees of freedom, a measure of the complexity of each term, F is the test statistic and $p$ the *p* value of the null hypothesis of a flat constant function or functions.
#| tbl-colwidths: [30,25,5,15,15,10]
overview(pw_chosen) |>
  select(-type) |>
  add_column(
    "Model term" = c(
      "$f(\\mathtt{day})$",
      "$f_{a(i)}(\\mathtt{day})$"
    ),
    .before = 1L
  ) |>
  rename(
    "Label" = term,
    `$k$` = k,
    EDF = edf,
    F = statistic,
    `$p$` = "p.value"
  ) |>
  knitr::kable(caption = NA, digits = 3)
```

## Quail hormone experiment {.unnumbered}

```{r}
#| label: fig-quail-data
#| fig-cap: todo
#| fig-width: 8
#| fig-height: 4
fn <- "data/Sarraude et al. - THs elevation in Japanese quails - dataset.xlsx"

# list sheets in workbook
# excel_sheets(fn)

q_growth <- read_xlsx(fn, sheet = "growth analysis", na = "NA") |>
  mutate(
    mother = factor(motherID),
    egg = factor(eggID),
    group = factor(group),
    sex = factor(sex)
  ) |>
  # make the label prettier for graphs
  mutate(
    sex = fct_recode(
      sex, "Female" = "F", "Male" = "M"
    ),
    group = fct_recode(
      group, "T[3]" = "T3", "T[4]" = "T4", "T[3]~T[4]" = "T3T4"
    )
  ) |>
  rename(treat = group) |>
  mutate(
    treat = fct_relevel(treat, "CO", "T[4]", "T[3]", "T[3]~T[4]")
  )

q_growth_plt <- q_growth |>
  ggplot(
    aes(
      day,
      mass,
      color = treat
    )
  ) +
    geom_line(aes(group = egg)) +
    facet_grid(sex ~ treat, labeller = label_parsed) +
    labs(x = "Days since hatching",
         y = "Body mass (g)",
         color = "Treatment") +
    scale_color_okabe_ito(
      labels = str2expression,
      limits = c("CO", "T[3]", "T[4]", "T[3]~T[4]"),
      order = c(1, 2, 3, 7)
    ) +
  theme(
    legend.text = element_text(hjust = 0),
    legend.position = "bottom"
  )
```

```{r}
#| label: quail-model-fits
#| cache: true
# bam() fits
use_discrete <- TRUE
quail_b1 <- bam(
  mass ~ s(day) +
    s(day, sex, bs = "sz") +
    s(day, egg, bs = "fs", k = 6) +
    s(mother, bs = "re"),
  data = q_growth,
  family = Gamma(link = "log"),
  method = "fREML",
  discrete = use_discrete,
  nthreads = 8
)

quail_b2 <- bam(
  mass ~ s(day) +
    s(day, treat, bs = "sz") +
    s(day, sex, bs = "sz") +
    s(day, egg, bs = "fs", k = 6) +
    s(mother, bs = "re"),
  data = q_growth,
  family = Gamma(link = "log"),
  method = "fREML",
  discrete = use_discrete,
  nthreads = 8
)

quail_b3 <- bam(
  mass ~ s(day) +
    s(day, treat, bs = "sz") +
    s(day, sex, bs = "sz") +
    s(day, treat, sex, bs = "sz") +
    s(day, egg, bs = "fs", k = 6) +
    s(mother, bs = "re"),
  data = q_growth,
  family = Gamma(link = "log"),
  method = "fREML",
  discrete = use_discrete,
  nthreads = 8
)

quail_b4 <- bam(
  mass ~ s(day) +
    s(day, treat, bs = "sz") +
    s(day, sex, bs = "sz") +
    s(day, treat, sex, bs = "sz") +
    s(egg, bs = "re") +
    s(mother, bs = "re"),
  data = q_growth,
  family = Gamma(link = "log"),
  method = "fREML",
  discrete = use_discrete,
  nthreads = 8
)

# update these to Tweedie models
quail_tw1 <- update(quail_b1, family = tw())
quail_tw2 <- update(quail_b2, family = tw())
quail_tw3 <- update(quail_b3, family = tw())
quail_tw4 <- update(quail_b4, family = tw())
```

As assessed by AIC, models containing treatment effects resulted in no improvement in the model fit (@tbl-quail-summary), a result that is consistent with the findings of @Sarraude2020-cw. However, the results of model Q3 are reported below as this model is consistent with the *a priori* assumption of treatment effects, possibly varying by sex, that underlay the original experiment. Using AIC to perform model selection would invalidate subsequent statistical inference we might wish to conduct on the fitted model. @fig-quail-data-plus-fits shows the original data and the estimated growth curves for each individual quail that were obtained from model Q3. The need for quail-specific random smooths is clear; model Q4 had the same model structure as that of Q3, except for the replacement of the quail-specific random smooths with quail-specific intercepts, which resulted in an increase in AIC of over 500 units (@tbl-quail-summary).

```{r}
#| label: fig-quail-data-plus-fits
#| fig-cap: Measured body mass (g) for 57 Japanese quail from an experiment on the effects of exposure to the maternal hormone $\text{T}_{4}$, its active metabolite $\text{T}_{3}$, both ($\text{T}_{3}\text{T}_{4}$) or a saline solution control (CO). The data (points) are shown along side the estimated growth curve for each quail obtained using generalized additive model Q3, which are shown by the coloured lines in each panel. The coloured shaded ribbon is the 95% bayesian credible interval around the estimate curve. The data are faceted by treatment and the sex of bird.
#| fig-width: 8
#| fig-height: 4
quail_lookup <- q_growth |>
  distinct(egg, mother, sex, treat)

quail_ds <- quail_tw3 |>
  data_slice(
    day = evenly(day),
    egg = evenly(egg)
  ) |>
  select(-mother, -sex, -treat) |>
  left_join(
    quail_lookup,
    by = join_by(egg == egg)
  )

quail_fv <- quail_tw3 |>
  fitted_values(
    data = quail_ds
  )

quail_fv |>
  ggplot(
    aes(
      x = day,
      y = .fitted,
      colour = treat,
      group = egg
    )
  ) +
  geom_point(
    data = q_growth,
    aes(
      x = day, y = mass, colour = treat
    ),
    size = 1
  ) +
  geom_ribbon(
    aes(ymin = .lower_ci, ymax = .upper_ci, fill = treat, colour = NULL),
    alpha = 0.2
  ) +
  geom_line() +
  facet_grid(sex ~ treat, labeller = label_parsed) +
  scale_color_okabe_ito(
    labels = str2expression,
    limits = c("CO", "T[3]", "T[4]", "T[3]~T[4]"),
    order = c(1, 2, 3, 7)
  ) +
  scale_fill_okabe_ito(
    labels = str2expression,
    limits = c("CO", "T[3]", "T[4]", "T[3]~T[4]"),
    order = c(1, 2, 3, 7)
  )  +
  labs(
    x = "Days since hatching",
    y = "Body mass (g)",
    color = "Treatment",
    fill = "Treatment",
  ) +
  theme(
    legend.text = element_text(hjust = 0),
    legend.position = "bottom"
  )
```

```{r}
#| label: tbl-quail-summary
#| tbl-cap: "Comparison of models fitted to the quail hormone experimental data set. The model label is shown (see text for the specific model formulations). Effective degrees of freedom represents model complexity in terms of the (effective) number of parameters in each model. Akaike's An information criterion (AIC) values, model deviance, and deviance explained as a proportion are also reported."
glance(quail_tw1) |>
  bind_rows(
    glance(quail_tw2),
    glance(quail_tw3),
    glance(quail_tw4)
  ) |>
  select(
    c("df", "AIC", "deviance")
  ) |>
  add_column(
    "Model" = paste0("Q", 1:4), .before = 1L
  ) |>
  add_column(
    "Deviance expl." = c(
      1 - (deviance(quail_tw1) / null_deviance(quail_tw1)),
      1 - (deviance(quail_tw2) / null_deviance(quail_tw2)),
      1 - (deviance(quail_tw3) / null_deviance(quail_tw3)),
      1 - (deviance(quail_tw4) / null_deviance(quail_tw4))
    )
  ) |>
  rename(
    EDF = df,
    Deviance = deviance
  ) |>
  knitr::kable(caption = NA, digits = 3)
```

To focus on the estimated treatment effects, model Q3 was evaluated at 100 evenly-spaced values over the time covariate (the number of evaluation points chosen to obtain a visually smooth representation of the estimated functions), plus all combinations of treatment level and sex. The effects of the quail-specific random smooths and the maternal random effects were excluded from these estimates. This results in estimated treatment effects for the average quial, which, strictly speaking should not be interpretted as population level effects due to the non-identity link function. The resulting estimated effects are shown in @fig-quail-tweedie-treatment-comparisons. While there are clear differences in the growth curves based on sex, there appears to be little qualitative difference in the effects of treatment levels on quail growth.

```{r}
#| label: fig-quail-tweedie-treatment-comparisons
#| fig-width: 8
#| fig-height: 3
#| fig-cap: Conditional value plots showing the growth rate for an average quail in each of the treatment groups by sex. The panels show the estimated curves for a particular treatment group; saline solution controls (CO), maternal hormone metabolite $\text{T}_{3}$, the maternal hormone $\text{T}_{4}$, and a combination of both $\text{T}_{3}\text{T}_{4}$. The estimated curve for male birds is shown by the solid line, and females the dashed line in each panel. The shaded band around each curve is a 95% bayesian credible interval.
#| cache: false

quail_tw3 |>
  conditional_values(
    condition = c("day", "treat", "sex"),
    exclude = c("s(day,egg)", "s(mother)")
  ) |>
  ggplot(
    aes(
      x = day,
      y = .fitted,
      colour = treat,
      linetype = sex
    )
  ) +
  geom_line() +
  geom_ribbon(
    aes(ymin = .lower_ci, ymax = .upper_ci, fill = treat, colour = NULL),
    alpha = 0.2
  ) +
  facet_wrap(~ treat, labeller = label_parsed, ncol = 4) +
  scale_color_okabe_ito(
    labels = str2expression,
    limits = c("CO", "T[3]", "T[4]", "T[3]~T[4]"),
    order = c(1, 2, 3, 7)
  ) +
  scale_fill_okabe_ito(
    labels = str2expression,
    limits = c("CO", "T[3]", "T[4]", "T[3]~T[4]"),
    order = c(1, 2, 3, 7)
  )  +
  labs(
    x = "Days since hatching",
    y = "Body mass (g)",
    color = "Treatment",
    fill = "Treatment",
    linetype = "Sex",
  ) +
  theme(
    legend.text = element_text(hjust = 0),
    legend.position = "bottom"
  )
```

The statistical summary of model Q3 is shown in @tbl-quail-weight-model-terms. The omnibus tests of the treatment specific deviations, and the treatment by sex deviations from the average curve, both have $p > 0.05$, which further reinforces the previously made observations that the effects of exposure to maternal thyroid hormone, if any, are small relative to the uncertainty in the model itself. The $p$ value for the omnibus test of the quail-specific deviation smooths ($f_{\text{egg}(i)}(\mathtt{day}_i)$) is also greater than 0.05. This is somewhat surprising, given the magnitude of the difference in AIC that is observed when these quail-specific random smooths are replace by a simple random intercept term ($\Delta \text{AIC} = \text{`r abs(AIC(quail_tw3) - AIC(quail_tw4)) |> round(3)`}$). Despite their lacking "statistical significance", removing these terms on the basis of $p$ values would lead to invalid inference.

The largest contribution to the overall EDF of this model (EDF = `r quail_tw3 |> model_edf() |> pull(.edf) |> round(3)`) is from the quail-specific deviation smooths (EDF = `r quail_tw3 |> edf(select = "s(day,egg)") |> pull(.edf) |> round(3)`), although as there are 57 individual animals and the random smooths include random intercepts to acocunt for differences between the average weight of each quail, this is unsurprising. Again, we see the effect of the wiggliness penalty, which has penalised the smooths back to EDF = `r quail_tw3 |> edf(select = "s(day,egg)") |> pull(.edf) |> round(3)` from a maximum of EDF of `r 57*6`.

```{r}
#| label: tbl-quail-weight-model-terms
#| tbl-colwidths: [30,25,5,15,15,10]
#| tbl-cap: Model summary for model Q3 fitted to the quail hormone experimental data set. The model term for comparison with the descriptions in the text and the label reported by the software are both shown for clarity. $k$ is the number of basis functions *per smooth*, EDF is the effective degrees of freedom, a measure of the complexity of each term, F is the test statistic and $p$ the *p* value of the null hypothesis of a flat constant function or functions.

overview(quail_tw3) |>
  select(-type) |>
  add_column(
    "Model term" = c(
      "$f(\\mathtt{day})$",
      "$f_{\\text{treat(i)}}(\\mathtt{day}_i)$",
      "$f_{\\text{sex(i)}}(\\mathtt{day}_i)$",
      "$f_{\\text{treat}(i),\\text{sex}(i)}(\\mathtt{day}_i)$",
      "$f_{\\text{egg}(i)}(\\mathtt{day}_i)$",
      "$\\xi_{\\text{mother}(i)}$"
    ),
    .before = 1L
  ) |>
  rename(
    "Label" = term,
    `$k$` = k,
    EDF = edf,
    F = statistic,
    `$p$` = "p.value"
  ) |>
  knitr::kable(caption = NA, digits = 3)
```

```{r}
#| label: quail-marginaleffects
#| cache: true

# test two things as illustration:
#   1. the estimated growth rate at t = 20 days, and
#   2. difference of means among treatments within sex at t = 70 days

# 1. estimate slopes at t = 20
slp_20 <- slopes(
  quail_tw3, 
  newdata = datagrid(day = 20, treat= unique, sex = unique),
  variables = "day", by = c("sex", "treat"), type = "response",
    exclude = c("s(day,egg)", "s(mother)")
)

slp_20_h <- slopes(
  quail_tw3, 
  newdata = datagrid(day = 20, treat = unique, sex = unique),
  variables = "day", by = c("sex", "treat"), type = "response",
  exclude = c("s(day,egg)", "s(mother)"),
  hypothesis = difference ~ pairwise | sex
)

hyp_20 <- hypotheses(
  slp_20_h,
  multcomp = "BY"
)

# 2. Estimate means, predictions, among treatment groups at t = 78 days, and
#.   test, pairwise within sex for treatment differences?
prd_78 <- predictions(
  quail_tw3, 
  newdata = datagrid(day = 78, treat = unique, sex = unique),
  variables = "day", by = c("sex", "treat"), type = "response",
  exclude = c("s(day,egg)", "s(mother)")
)

prd_78_h <- predictions(
  quail_tw3, 
  newdata = datagrid(day = 78, treat = unique, sex = unique),
  variables = "day", by = c("sex", "treat"), type = "response",
  exclude = c("s(day,egg)", "s(mother)"),
  hypothesis = difference ~ pairwise | sex
)

hyp_78 <- hypotheses(
  prd_78_h,
  multcomp = "BY"
)
```

To formally assess the differences among treatment effects, pairwise comparisons of treatment levels within sex were conducted for the slope of the average growth curve at day 20. The estimated differences in the slopes of the growth curves are shown in @tbl-hyp-20. For all comparisons, despite some observed differences in the growth rates at day 20 among the treatment levels, the 95% credible intervals include zero for all comparisons. Therefore, if these results reflect the broader population of quail, we should expect that any differences in growth rate due to maternal hormones are small, and largely indistinguishable from zero.

```{r}
#| label: tbl-hyp-20
#| tbl-cap: Pairwise comparisons of treatment effects by sex on the slopes of the growth curves at day 20 for an average quail of the indicated sex in the pair of treatments shown. The Hypothesis column lists the specific pairwise comparison, Diff. is the estimated difference in the slope of the growth curves compared, SE its standard error. Z is the Wald test statistic, $p$ its *p* value, and associated endpoints of a bayesian 95% credible interval on the estimated difference of slopes.
#| tbl-colwidths: [15,30,10,10,10,5,10,10]

hyp_20 |>
  as_tibble() |>
  select(
    -c(term, s.value)
  ) |>
  mutate(
    p.value = format.pval(p.value, eps = 0.001, digits = 3),
    hypothesis = case_match(
      hypothesis,
      "(T[3]) - (CO)"        ~ "$\\text{T}_{3} - \\text{Control}$",
      "(T[4]) - (CO)"        ~ "$\\text{T}_{4} - \\text{Control}$",
      "(T[3]~T[4]) - (CO)"   ~ "$\\text{T}_{3} \\; \\text{T}_{4} - \\text{Control}$",
      "(T[3]~T[4]) - (T[3])" ~ "$\\text{T}_{3} \\; \\text{T}_{4} - \\text{T}_{3}$",
      "(T[4]) - (T[3])"      ~ "$\\text{T}_{4} - \\text{T}_{3}$",
      "(T[4]) - (T[3]~T[4])" ~ "$\\text{T}_{4} - \\text{T}_{3} \\; \\text{T}_{4}$"
    )
  ) |>
  rename(
    Sex = sex,
    Hypothesis = hypothesis,
    "Diff." = estimate,
    SE = "std.error",
    Z = statistic,
    "$p$" = p.value,
    "2.5%" = conf.low,
    "97.5%" = conf.high
  ) |>
  relocate(
    Sex,
    .before = 1L
  ) |>
  knitr::kable(caption = NA, digits = 3)
```

Finally, pairwise comparisons of quail weight at the end of the experiment (day 78) among treatment levels within sex were used to examine differences in estimated weight of quail due to exposure to maternal thyroid hormones. The results of these comparisons are shown in @tbl-hyp-78. The largest observed difference (~10.1 g) in weight of an average quail was between the $\text{T}_4$ hormone treatment and controls in female quail. This represents approximately a 5% decrease in the weight of an average female quail when exposed to the $\text{T}_4$ hormone compared with the untreated controls. Yet, given the uncertainty in the estimates of the model coefficients, the 95% credible interval includes 0 for this, and all other, comparisons.

```{r}
#| label: tbl-hyp-78
#| tbl-cap: Pairwise comparisons of treatment effects by sex on the estimated mean of the quail growth curves at day 78 for an average quail of the indicated sex in each pair of treatments. The Hypothesis column lists the specific pairwise comparison, Diff. is the estimated difference of mean body mass (g) of the growth curves compared, SE its standard error. Z is the Wald test statistic, $p$ its *p* value, and associated endpoints of a bayesian 95% credible interval on the estimated difference of means.
#| tbl-colwidths: [10,25,14,8,10,2,15,14]

hyp_78 |>
  as_tibble() |>
  left_join(
    prd_78_h |>
      as_tibble() |>
      mutate(
        term = paste0("b", row_number())
      ) |>
      select(c(term, sex)),
    by = join_by("term")
  ) |>
  select(
    -c(term, s.value)
  ) |>
  mutate(
    p.value = format.pval(p.value, eps = 0.001, digits = 3),
    hypothesis = case_match(
      hypothesis,
      "(T[3]) - (CO)"        ~ "$\\text{T}_{3} - \\text{Control}$",
      "(T[4]) - (CO)"        ~ "$\\text{T}_{4} - \\text{Control}$",
      "(T[3]~T[4]) - (CO)"   ~ "$\\text{T}_{3} \\text{T}_{4} - \\text{Control}$",
      "(T[3]~T[4]) - (T[3])" ~ "$\\text{T}_{3} \\text{T}_{4} - \\text{T}_{3}$",
      "(T[4]) - (T[3])"      ~ "$\\text{T}_{4} - \\text{T}_{3}$",
      "(T[4]) - (T[3]~T[4])" ~ "$\\text{T}_{4} - \\text{T}_{3} \\text{T}_{4}$"
    )
  ) |>
  rename(
    Sex = sex,
    Hypothesis = hypothesis,
    "Diff." = estimate,
    SE = "std.error",
    Z = statistic,
    "$p$" = p.value,
    "2.5%" = conf.low,
    "97.5%" = conf.high
  ) |>
  relocate(
    Sex,
    .before = 1L
  ) |>
  knitr::kable(caption = NA, digits = 3)
```

# Author' Points of View

The results presented above, clearly demonstrate the utility of GAMs for modelling the kinds of data commonly encountered in research involving animals. GAMs are often viewed unfavourably as being subjective, requiring the user to specify how complex they want the estimated smooth functions to be, data hungry, and more of a data visualisation tool ill-suited to formal statistical inference. These views, while perhaps valid in the case of the subjectivity critique, are largely out-dated with modern GAMs as I have presented above. The subjectivity critique has been addressed through developments in automated smoothness selection. None of the data sets analysed here are especially large, certainly not given today's standards, and much research has been done [e.g., @Li2020-ch] and continues to be done to adapt the algorithms to ever higher dimensional problems. The `bam()` function in the *mgcv* package, for example, can handle data on the order of millions of rows, with tens of thousands of model parameters, given sufficient availablity of computer memory.

As demonstrated in the quail maternal hormone example, formal statistical inference is entirely feasible. If we ignore the selection of smoothness parameters, the models described here are little more than generalized linear (mixed) models (GL(M)Ms) once the basis expansions of covariates have been performed. Modern software tools like the *marginaleffects* package for R allow a consistent interface to statistical inference across many disparate statistical models, and GAMs are no different in this regard.

The distinct advantage of GAMs over GL(M)Ms is their ability to learn the shapes of nonlinear relationships between covariates and the repsonse from the data themselves. This relieves the analyst from having to force data to fit a particular theoretical model, unless they have a good justification to use that model of course. GAMs also avoid the model selection problems inherent to modelling with polynomial basis expansions (e.g. $x + x^2 + x^3 + \cdots + x^p$).

The main disadvantage of GAMs is that they are more complex to fit than GL(M)Ms; the analyst has some additional data modelling choices to make when using GAMs. The main choice is the number of basis functions, $K$, that should be used by each smooth in the model. The general advice here is for the analyst to imagine the largest amount of wiggliness that they would expect and then set $k$ a little larger than this. Of course, the novice user of GAMs will lack the experience required to do this easily, but for the sorts of data problems exemplified above, we would not expect highly complex smooth functions, and the default of $k$ = 10 for univariate smooths in *mgcv* is usually sufficient for many problems. The key requirement is that the initial number of basis functions used should be large enough such that the span of functions representable with that basis will contain the true but unknown function or a close approximation to it. The basis dimension must be checked of course, and this adds another step to the model appraisal or checking procedure. With *mgcv* for example, the `k.check()` function provides a test for sufficiency of the basis size used to fit the model [@Pya2016-rk].

A further disadvantage is that statistical inference is somewhat more approximate that with GL(M)Ms. While GAMs share with GL(M)Ms the property of having asymptotically correct *p* values, the *p* values for smooths are more approximate than for terms in a GL(M)M because the current theory on which these tests are based does not account for the selection of smoothing parameters; for the purposes of the tests, the smoothing parameters are treated as being fixed and known, but instead they are estimated from the data [@Wood2013-fb]. While there has been some progress in adapting the theory to include this additional source of uncertainty [e.g., @Wood2016-fx], as yet this has not been used to correct the *p* values of tests of smooths.

Finally, GAMs can require more substantial amounts of computing resources to fit than GL(M)Ms once data sets get above tens of thousand observations or where models include several or complex random effect terms, using the algorithms provided by *mgcv*. The main requirement is computer memory, although the `bam()` function can help with this using algorithmic improvements from @Li2020-ch. The examples above were all run on an Apple M1 Pro MacBook Pro with 32GB of RAM, and the entire analysis including the generations of figures takes only a few minutes.

GAMs are a very broad and general class of models. In the case of the pig growth data, several animals exhibited visibly more variation in their weight measurements than the majority of the animals in the data. Such heteroscedasticity (non-constant variance) can be modelled using distributional GAMs (or Generalized additive models for location, scale, shape or GAMLSS) [e.g. @Rigby2005-nl; @Kneib2021-zb; @Klein2024-im], which include linear predictors for all of the parameters of a distribution, or centile or quantile models [e.g., @Nakamura2022-nf]. In the examples above, the additional parameters (scale in the case of the Gamma, or the Tweedie power parameter) were estimated as constants for the entire data set. A distributional GAM would allow those parameters to potentially vary with individual animals, or as as smooth functions of the covariates, just as was done for the mean of the distribution in this study.

In conclusion, GAMs are a modern, flexible, and highly usable statistical model that is amenable to many research problems in animal science, and deserve a place in the statistical toolbox.

# Ethics approval

Not applicable. See the original sources of the data used for ethical approvement.

# Declaration of Generative AI and AI-assisted technologies in the writing process

The author did not use any artifical intelligence technologies in the writing process.

# Author ORCIDs

**Gavin L. Simpson**: 0000-0002-9084-8413

# Author contributions

GLS: Conceptualization, Methodology, Software, Formal analysis, Writing, Visulaization.

# Declaration of interest

None.

# Acknowledgements

The author would to like to express their appreciation to colleagues at the Department of Animal and Veterinary Sciences, Aarhus University for making the pig growth data available, and in particular to Dr. Mona Larsen for supplying the data and arranging with their coauthors to enable the subset analysed in this manuscript to be made available as open data.

# Financial support statement

This work was supported by an Aarhus Universitets Forskningsfond (Aarhus University Research Foundation; AUFF) starting grant awarded to the author.

# References

::: {#refs}

:::

\newpage

\processdelayedfloats

# Figure captions

Fig. 1: Illustration of how penalised splines work. A spline basis expansion (a) and associated penalty matrix $\mathbf{S}$ (c) are formed for a covariate $x$. Model fitting involves finding estimates for the coefficients of the basis functions that make the fitted spline (thick, blue curve) go as close to the data (black points) as possible, without over fitting (b). In (a) and (b) the basis functions are shown as thin coloured lines and are from a B spline basis. The sum-to-zero identifiability constraint needed so that an intercept can be included in the model has been absorbed into the basis shown. The dashed horizontal line in (b) is the estimated value of the intercept. The penalty matrix (c) encodes how wiggly each basis function is in terms of its second derivative.

Fig. 2: Illustration of how the wiggliness penalty controls the resulting fit of a penalised spline. The weighted basis functions are shown as thin coloured lines. In each panel a penalised spline is shown by the solid black line, which has been fitted to the data points shown. The wiggliness value of the spline, the integrated squared derivative of the fitted spline over $x$ is given in the upper right of each panel. The spline in (a) is over fitted to the data, resulting in a very wiggly function with a large wiggliness value. The spline in (c) is over smoothed, resulting in a a simple fitted function with low wiggliness, but which does not fit the data well. The spline in (b) represents a balance between fit to the data and complexity of fitted function. The smoothing parameter for the spline, $\lambda$, is used as a tuning parameter in the model, which ultimately controls this balance between fit and complexity.

Fig. 3: Results of model fitting to the average daily fat content data from @Henderson1990-bd. a) observed average daily fat content (points) and estimated lactation curves from Wood's [-@Wood1967-re] model, a Tweedie GLM, and a Tweedie GAM (lines) with associated 95% confidence (Wood's model) or 95% credible intervals (GLM and GAM). Response residuals for Wood's model (b), Tweedie GLM (c), and Tweedie GAM (d), plus scatter plot smoothers (lines) and 95% credible intervals (shaded ribbons).

Fig. 4: Depth camera-based weight estimates from 18 commerical pigs. The data (black points) are the average of multiple measurements taken of each animal per day, 1 panel per pig. The panel labels indicate to the pig shown. The estimated growth curve for each pig obtained using generalized additive model P2 is shown by the blue line in each panel. The blue shaded ribbon is the 95% bayesian credible interval around the estimate curve.

Fig. 5: Measured body mass (g) for 57 Japanese quail from an experiment on the effects of exposure to the maternal hormone $\text{T}_{4}$, its active metabolite $\text{T}_{3}$, both ($\text{T}_{3}\text{T}_{4}$) or a saline solution control (CO). The data (points) are shown along side the estimated growth curve for each quail obtained using generalized additive model Q3, which are shown by the coloured lines in each panel. The coloured shaded ribbon is the 95% bayesian credible interval around the estimate curve. The data are faceted by treatment and the sex of bird.

Fig. 6: Conditional value plots showing the growth rate for an average quail in each of the treatment groups by sex. The panels show the estimated curves for a particular treatment group; saline solution controls (CO), maternal hormone metabolite $\text{T}_{3}$, the maternal hormone $\text{T}_{4}$, and a combination of both $\text{T}_{3}\text{T}_{4}$. The estimated curve for male birds is shown by the solid line, and females the dashed line in each panel. The shaded band around each curve is a 95% bayesian credible interval.
